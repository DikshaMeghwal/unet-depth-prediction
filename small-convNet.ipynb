{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import zipfile\n",
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "output_height=64\n",
    "output_width=64\n",
    "\n",
    "class TransposeDepthInput(object):\n",
    "    def __call__(self, depth):\n",
    "        depth = depth.transpose((2, 0, 1))\n",
    "        depth = torch.from_numpy(depth)\n",
    "        depth = depth.view(1, depth.shape[0], depth.shape[1], depth.shape[2])\n",
    "        depth = nn.functional.interpolate(depth, size=(output_height, output_width), mode='bilinear', align_corners=False)\n",
    "        depth = torch.log(depth[0])\n",
    "        return depth\n",
    "\n",
    "rgb_data_transforms = transforms.Compose([\n",
    "    transforms.Resize((output_height, output_width)),    # Different for Input Image & Depth Image\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "depth_data_transforms = transforms.Compose([\n",
    "    TransposeDepthInput(),\n",
    "])\n",
    "\n",
    "input_for_plot_transforms = transforms.Compose([\n",
    "    transforms.Resize((output_height, output_width)),    # Different for Input Image & Depth Image\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class NYUDataset(Dataset):\n",
    "    def __init__(self, filename, type, rgb_transform = None, depth_transform = None):\n",
    "        f = h5py.File(filename, 'r')\n",
    "        if type == \"training\":\n",
    "            self.images = f['images'][0:1024]\n",
    "            self.depths = f['depths'][0:1024]\n",
    "        elif type == \"validation\":\n",
    "            self.images = f['images'][1024:1248]\n",
    "            self.depths = f['depths'][1024:1248]\n",
    "        elif type == \"test\":\n",
    "            self.images = f['images'][1248:]\n",
    "            self.depths = f['depths'][1248:]\n",
    "        self.rgb_transform = rgb_transform\n",
    "        self.depth_transform = depth_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = image.transpose((2, 1, 0))\n",
    "        image = Image.fromarray(image)\n",
    "        if self.rgb_transform:\n",
    "            image = self.rgb_transform(image)\n",
    "        depth = self.depths[idx]\n",
    "        depth = np.reshape(depth, (1, depth.shape[0], depth.shape[1]))\n",
    "        depth = depth.transpose((2, 1, 0))\n",
    "        if self.depth_transform:\n",
    "            depth = self.depth_transform(depth)\n",
    "        sample = {'image': image, 'depth': depth}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n",
    "        self.conv1 = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.conv(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu, space_dropout=False):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_size, out_size, 2, stride=2)\n",
    "        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        out = torch.cat([up, bridge], 1)\n",
    "        out = self.activation(self.conv(out))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.activation = F.relu\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv_block1_64 = UNetConvBlock(3, 64)\n",
    "        self.conv_block64_128 = UNetConvBlock(64, 128)\n",
    "        self.conv_block128_256 = UNetConvBlock(128, 256)\n",
    "        self.conv_block256_512 = UNetConvBlock(256, 512)\n",
    "        self.conv_block512_1024 = UNetConvBlock(512, 1024)\n",
    "\n",
    "        self.up_block1024_512 = UNetUpBlock(1024, 512)\n",
    "        self.up_block512_256 = UNetUpBlock(512, 256)\n",
    "        self.up_block256_128 = UNetUpBlock(256, 128)\n",
    "        self.up_block128_64 = UNetUpBlock(128, 64)\n",
    "\n",
    "        self.last = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1_64(x)\n",
    "        pool1 = self.pool1(block1)\n",
    "\n",
    "        block2 = self.conv_block64_128(pool1)\n",
    "        pool2 = self.pool2(block2)\n",
    "\n",
    "        block3 = self.conv_block128_256(pool2)\n",
    "        pool3 = self.pool3(block3)\n",
    "\n",
    "        block4 = self.conv_block256_512(pool3)\n",
    "        pool4 = self.pool4(block4)\n",
    "\n",
    "        block5 = self.conv_block512_1024(pool4)\n",
    "\n",
    "        up1 = self.up_block1024_512(block5, block4)\n",
    "\n",
    "        up2 = self.up_block512_256(up1, block3)\n",
    "\n",
    "        up3 = self.up_block256_128(up2, block2)\n",
    "\n",
    "        up4 = self.up_block128_64(up3, block1)\n",
    "\n",
    "        return self.last(up4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Training the Unet Model **************\n",
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 4.387237\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 2 [0/1024 (0%)]\tLoss: 4.335034\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 3 [0/1024 (0%)]\tLoss: 4.284197\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 4 [0/1024 (0%)]\tLoss: 4.232645\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 5 [0/1024 (0%)]\tLoss: 4.177440\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 6 [0/1024 (0%)]\tLoss: 4.117635\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 7 [0/1024 (0%)]\tLoss: 4.053560\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 8 [0/1024 (0%)]\tLoss: 3.985512\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 9 [0/1024 (0%)]\tLoss: 3.911790\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 10 [0/1024 (0%)]\tLoss: 3.830481\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 11 [0/1024 (0%)]\tLoss: 3.739851\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 12 [0/1024 (0%)]\tLoss: 3.638896\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 13 [0/1024 (0%)]\tLoss: 3.526835\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 14 [0/1024 (0%)]\tLoss: 3.400808\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 15 [0/1024 (0%)]\tLoss: 3.257618\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 16 [0/1024 (0%)]\tLoss: 3.094660\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 17 [0/1024 (0%)]\tLoss: 2.907321\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 18 [0/1024 (0%)]\tLoss: 2.688883\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 19 [0/1024 (0%)]\tLoss: 2.428829\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 20 [0/1024 (0%)]\tLoss: 2.109975\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from logger import Logger\n",
    "import pdb\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Training settings\n",
    "# parser = argparse.ArgumentParser(description='PyTorch depth map prediction example')\n",
    "# parser.add_argument('model_folder', type=str, default='trial', metavar='F',\n",
    "#                      help='In which folder do you want to save the model')\n",
    "# parser.add_argument('--data', type=str, default='data', metavar='D',\n",
    "#                      help=\"folder where data is located. train_data.zip and test_data.zip need to be found in the folder\")\n",
    "# parser.add_argument('--batch-size', type = int, default = 32, metavar = 'N',\n",
    "#                      help='input batch size for training (default: 8)')\n",
    "# parser.add_argument('--epochs', type=int, default = 10, metavar='N',\n",
    "#                       help='number of epochs to train (default: 10)')\n",
    "# parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "#                      help='learning rate (default: 0.01)')\n",
    "# parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "#                      help='SGD momentum (default: 0.5)')\n",
    "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                      help='random seed (default: 1)')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                      help='how many batches to wait before logging training status')\n",
    "# parser.add_argument('--suffix', type=str, default='', metavar='D',\n",
    "#                      help='suffix for the filename of models and output files')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "data = 'data'\n",
    "batch_size = 8\n",
    "epochs = 20\n",
    "lr = 0.0001\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "suffix = ''\n",
    "model_folder = 'local-unet'\n",
    "\n",
    "#torch.manual_seed(seed)\n",
    "\n",
    "### Data Initialization and Loading\n",
    "# from data import initialize_data, rgb_data_transforms, depth_data_transforms, output_height, output_width\n",
    "#initialize_data(args.data) # extracts the zip files, makes a validation set\n",
    "\n",
    "# from data import NYUDataset, rgb_data_transforms, depth_data_transforms, input_for_plot_transforms, output_height, output_width\n",
    "\n",
    "# train_rgb_loader = torch.utils.data.DataLoader(datasets.ImageFolder(args.data + '/train_images/rgb/', transform = rgb_data_transforms), batch_size=args.batch_size, shuffle=False, num_workers=1)\n",
    "# train_depth_loader = torch.utils.data.DataLoader(datasets.ImageFolder(args.data + '/train_images/depth/', transform = depth_data_transforms), batch_size=args.batch_size, shuffle=False, num_workers=1)\n",
    "# val_rgb_loader = torch.utils.data.DataLoader(datasets.ImageFolder(args.data + '/val_images/rgb/', transform = rgb_data_transforms), batch_size=args.batch_size, shuffle=False, num_workers=1)\n",
    "# val_depth_loader = torch.utils.data.DataLoader(datasets.ImageFolder(args.data + '/val_images/depth/', transform = depth_data_transforms), batch_size=args.batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(NYUDataset( 'nyu_depth_v2_labeled.mat', \n",
    "                                                       'training', \n",
    "                                                        rgb_transform = rgb_data_transforms, \n",
    "                                                        depth_transform = depth_data_transforms), \n",
    "                                                        batch_size = batch_size, \n",
    "                                                        shuffle = False, num_workers = 0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(NYUDataset( 'nyu_depth_v2_labeled.mat',\n",
    "                                                     'validation', \n",
    "                                                     rgb_transform = rgb_data_transforms, \n",
    "                                                     depth_transform = depth_data_transforms), \n",
    "                                                     batch_size = batch_size, \n",
    "                                                     shuffle = False, num_workers = 0)\n",
    "\n",
    "from model import UNet\n",
    "model = UNet()\n",
    "# model.cuda()\n",
    "\n",
    "def rel_error(output, target):\n",
    "    target = target + 0.000001\n",
    "    target = log10(target)\n",
    "    output = output + 0.000001\n",
    "    output = log10(output)\n",
    "    return F.mse_loss(output, target)\n",
    "    #diff = (output-target)/target\n",
    "    #diff = torch.abs(diff)\n",
    "    #return diff.mean()\n",
    "\n",
    "def custom_loss_function(output, target):\n",
    "    # di = torch.log(target) - torch.log(output)\n",
    "    di = target - output\n",
    "    n = (output_height * output_width)\n",
    "    di2 = torch.pow(di, 2)\n",
    "    fisrt_term = torch.sum(di2,(1,2,3))/n\n",
    "    second_term = 0.5*torch.pow(torch.sum(di,(1,2,3)), 2)/ (n**2)\n",
    "    loss = fisrt_term - second_term\n",
    "    return loss.sum()\n",
    "\n",
    "loss_function = custom_loss_function\n",
    "#loss_function = F.mse_loss\n",
    "#loss_function = F.smooth_l1_loss\n",
    "#loss_function = rel_error\n",
    "optimizer = optim.Adam(model.parameters(), amsgrad=True, lr=0.0001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.0001, momentum=0.99)\n",
    "#optimizer = optim.Adamax(model.parameters())\n",
    "dtype=torch.cuda.FloatTensor\n",
    "logger = Logger('./logs/' + model_folder)\n",
    "\n",
    "def display_images(images):\n",
    "    grid = utils.make_grid(images)\n",
    "    plt.imshow(grid.cpu().detach().numpy().transpose((1, 2, 0)))\n",
    "    plt.show();\n",
    "\n",
    "def format_data_for_display(tensor):\n",
    "    maxVal = tensor.max()\n",
    "    minVal = abs(tensor.min())\n",
    "    maxVal = max(maxVal,minVal)\n",
    "    output_data = tensor / maxVal\n",
    "    output_data = output_data / 2\n",
    "    output_data = output_data + 0.5\n",
    "    return output_data\n",
    "\n",
    "def plot_grid(fig, plot_input, output, actual_output, row_no):\n",
    "        grid = ImageGrid(fig, 141, nrows_ncols=(row_no, 4), axes_pad=0.05, label_mode=\"1\")\n",
    "        for i in range(row_no):\n",
    "                for j in range(3):\n",
    "                        if(j == 0):\n",
    "                                grid[i*4+j].imshow(np.transpose(plot_input[i], (1, 2, 0)), interpolation=\"nearest\")\n",
    "                        if(j == 1):\n",
    "                                grid[i*4+j].imshow(np.transpose(output[i][0].detach().cpu().numpy(), (0, 1)), interpolation=\"nearest\")\n",
    "                        if(j == 2):\n",
    "                                grid[i*4+j].imshow(np.transpose(actual_output[i][0].detach().cpu().numpy(), (0, 1)), interpolation=\"nearest\")\n",
    "\n",
    "def train_Unet(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        rgb, depth = data['image'], data['depth']\n",
    "        optimizer.zero_grad()\n",
    "        output = model(rgb)\n",
    "        target = depth[:,0,:,:].view(list(depth.shape)[0], 1, output_height, output_width)\n",
    "        #print(\"target\")\n",
    "        #print(target)\n",
    "        #print(\"output\")\n",
    "        #print(output)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        F = plt.figure(1, (30, 60))\n",
    "        F.subplots_adjust(left=0.05, right=0.95)\n",
    "        plot_grid(F, rgb, target, output, batch_size)\n",
    "        plt.savefig(\"plots/train_\" + model_folder + \"_\" + str(epoch) + \"_\" + str(batch_idx) + \".jpg\")\n",
    "        plt.show()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            training_tag = \"training loss epoch:\" + str(epoch)\n",
    "            logger.scalar_summary(training_tag, loss.item(), batch_idx)\n",
    "\n",
    "            for tag, value in model.named_parameters():\n",
    "                tag = tag.replace('.', '/') + \":\" + str(epoch)\n",
    "                #logger.histo_summary(tag, value.data.cpu().numpy(), batch_idx)\n",
    "                #logger.histo_summary(tag + '/grad', value.grad.data.cpu().detach().numpy(), batch_idx)\n",
    "\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(rgb), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "#         batch_idx = batch_idx + 1\n",
    "        if batch_idx == 0: break\n",
    "\n",
    "def validate_Unet():\n",
    "    print('validating unet')\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(val_loader):\n",
    "            rgb, depth = data['image'], data['depth']\n",
    "            output = model(rgb)\n",
    "            target = depth[:,0,:,:].view(list(depth.shape)[0], 1, output_height, output_width)\n",
    "            validation_loss += rel_error(output, target)\n",
    "#           if batch_idx == 2: break\n",
    "            rel_loss = rel_error(output, target)\n",
    "            rms_loss = F.mse_loss(output, target)\n",
    "        validation_loss /= batch_idx\n",
    "        rel_loss /= batch_idx\n",
    "        rms_loss /= batch_idx\n",
    "        logger.scalar_summary(\"validation loss\", validation_loss, epoch)\n",
    "        print('\\nValidation set: Average loss: {:.6f} {:.6f} {:.6f}\\n'.format(validation_loss, rel_loss, rms_loss))\n",
    "\n",
    "folder_name = \"models/\" + model_folder\n",
    "if not os.path.exists(folder_name): os.mkdir(folder_name)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"********* Training the Unet Model **************\")\n",
    "    train_Unet(epoch)\n",
    "    if epoch % 25== 0:\n",
    "        model_file = folder_name + \"/\" + 'model_' + str(epoch) + '.pth'\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "#    validate_Unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
