{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def display_images(images):\n",
    "    grid = utils.make_grid(images)\n",
    "    plt.imshow(grid.cpu().detach().numpy().transpose((1, 2, 0)))\n",
    "    plt.show();\n",
    "\n",
    "def format_data_for_display(tensor):\n",
    "    maxVal = tensor.max()\n",
    "    minVal = abs(tensor.min())\n",
    "    maxVal = max(maxVal,minVal)\n",
    "    output_data = tensor / maxVal\n",
    "    output_data = output_data / 2\n",
    "    output_data = output_data + 0.5\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import zipfile\n",
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "output_height=64\n",
    "output_width=64\n",
    "\n",
    "class TransposeDepthInput(object):\n",
    "    def __call__(self, depth):\n",
    "        depth = depth.transpose((2, 0, 1))\n",
    "        depth = torch.from_numpy(depth)\n",
    "        depth = depth.view(1, depth.shape[0], depth.shape[1], depth.shape[2])\n",
    "        depth = nn.functional.interpolate(depth, size=(output_height, output_width), mode='bilinear', align_corners=False)\n",
    "        depth = torch.log(depth[0])\n",
    "        return depth\n",
    "\n",
    "rgb_data_transforms = transforms.Compose([\n",
    "    transforms.Resize((output_height, output_width)),    # Different for Input Image & Depth Image\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "depth_data_transforms = transforms.Compose([\n",
    "    TransposeDepthInput(),\n",
    "])\n",
    "\n",
    "input_for_plot_transforms = transforms.Compose([\n",
    "    transforms.Resize((output_height, output_width)),    # Different for Input Image & Depth Image\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class NYUDataset(Dataset):\n",
    "    def __init__(self, filename, type, rgb_transform = None, depth_transform = None):\n",
    "        f = h5py.File(filename, 'r')\n",
    "        if type == \"training\":\n",
    "            self.images = f['images'][0:1024]\n",
    "            self.depths = f['depths'][0:1024]\n",
    "        elif type == \"validation\":\n",
    "            self.images = f['images'][1024:1248]\n",
    "            self.depths = f['depths'][1024:1248]\n",
    "        elif type == \"test\":\n",
    "            self.images = f['images'][1248:]\n",
    "            self.depths = f['depths'][1248:]\n",
    "        self.rgb_transform = rgb_transform\n",
    "        self.depth_transform = depth_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = image.transpose((2, 1, 0))\n",
    "        image = Image.fromarray(image)\n",
    "        if self.rgb_transform:\n",
    "            image = self.rgb_transform(image)\n",
    "        depth = self.depths[idx]\n",
    "        depth = np.reshape(depth, (1, depth.shape[0], depth.shape[1]))\n",
    "        depth = depth.transpose((2, 1, 0))\n",
    "        if self.depth_transform:\n",
    "            depth = self.depth_transform(depth)\n",
    "        sample = {'image': image, 'depth': depth}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, padding=1, stride=2):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_size, out_size, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=2, stride=2, space_dropout=False):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "        self.conv_block3_16 = UNetConvBlock(3, 16)\n",
    "        self.conv_block16_32 = UNetConvBlock(16, 32)\n",
    "        self.conv_block32_64 = UNetConvBlock(32, 64)\n",
    "\n",
    "        self.up_block64_32 = UNetUpBlock(64, 32)\n",
    "        self.up_block32_16 = UNetUpBlock(32, 16)\n",
    "        self.up_block16_1 = UNetUpBlock(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.activation(self.conv_block3_16(x))\n",
    "        block2 = self.activation(self.conv_block16_32(block1))\n",
    "        block3 = self.activation(self.conv_block32_64(block2))\n",
    "        up1 = self.activation(self.up_block64_32(block3))\n",
    "        up2 = self.activation(self.up_block32_16(up1))\n",
    "        up3 = self.up_block16_1(up2)\n",
    "        return up3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "     UNetConvBlock-2           [-1, 16, 32, 32]               0\n",
      "            Conv2d-3           [-1, 32, 16, 16]           4,640\n",
      "     UNetConvBlock-4           [-1, 32, 16, 16]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          18,496\n",
      "     UNetConvBlock-6             [-1, 64, 8, 8]               0\n",
      "   ConvTranspose2d-7           [-1, 32, 16, 16]           8,224\n",
      "       UNetUpBlock-8           [-1, 32, 16, 16]               0\n",
      "   ConvTranspose2d-9           [-1, 16, 32, 32]           2,064\n",
      "      UNetUpBlock-10           [-1, 16, 32, 32]               0\n",
      "  ConvTranspose2d-11            [-1, 1, 64, 64]              65\n",
      "      UNetUpBlock-12            [-1, 1, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 33,937\n",
      "Trainable params: 33,937\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.88\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 1.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(NYUDataset( 'nyu_depth_v2_labeled.mat', \n",
    "                                                       'training', \n",
    "                                                        rgb_transform = rgb_data_transforms, \n",
    "                                                        depth_transform = depth_data_transforms), \n",
    "                                                        batch_size = batch_size, \n",
    "                                                        shuffle = False, num_workers = 0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(NYUDataset( 'nyu_depth_v2_labeled.mat',\n",
    "                                                     'validation', \n",
    "                                                     rgb_transform = rgb_data_transforms, \n",
    "                                                     depth_transform = depth_data_transforms), \n",
    "                                                     batch_size = batch_size, \n",
    "                                                     shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "torch.Size([1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c0b9af50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torchvision.utils import save_image\n",
    "# from IPython.core.display import Image, display\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "first_rgb = data['image'][0]\n",
    "first_depth = data['depth'][0]\n",
    "print(first_rgb.shape)\n",
    "print(first_depth.shape)\n",
    "# npimg = first_rgb.numpy()\n",
    "plt.imshow(np.transpose(first_rgb.numpy(), (1,2,0)), interpolation='nearest')\n",
    "# display_images(format_data_for_display(first_rgb))\n",
    "# display_images(format_data_for_display(first_depth))\n",
    "# save_image(first_rgb, 'real_image.png')\n",
    "\n",
    "# Image('real_image.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Training the Unet Model **************\n",
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 0.293890\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 2 [0/1024 (0%)]\tLoss: 0.287035\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 3 [0/1024 (0%)]\tLoss: 0.279640\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 4 [0/1024 (0%)]\tLoss: 0.271508\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 5 [0/1024 (0%)]\tLoss: 0.262513\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 6 [0/1024 (0%)]\tLoss: 0.252386\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 7 [0/1024 (0%)]\tLoss: 0.240640\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 8 [0/1024 (0%)]\tLoss: 0.226996\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 9 [0/1024 (0%)]\tLoss: 0.211149\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 10 [0/1024 (0%)]\tLoss: 0.192985\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 11 [0/1024 (0%)]\tLoss: 0.172089\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 12 [0/1024 (0%)]\tLoss: 0.148464\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 13 [0/1024 (0%)]\tLoss: 0.123130\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 14 [0/1024 (0%)]\tLoss: 0.098685\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 15 [0/1024 (0%)]\tLoss: 0.080164\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 16 [0/1024 (0%)]\tLoss: 0.075333\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 17 [0/1024 (0%)]\tLoss: 0.084601\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 18 [0/1024 (0%)]\tLoss: 0.088539\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 19 [0/1024 (0%)]\tLoss: 0.079641\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 20 [0/1024 (0%)]\tLoss: 0.065324\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 21 [0/1024 (0%)]\tLoss: 0.053148\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 22 [0/1024 (0%)]\tLoss: 0.046233\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 23 [0/1024 (0%)]\tLoss: 0.043923\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 24 [0/1024 (0%)]\tLoss: 0.044169\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 25 [0/1024 (0%)]\tLoss: 0.045151\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 26 [0/1024 (0%)]\tLoss: 0.045632\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 27 [0/1024 (0%)]\tLoss: 0.045077\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 28 [0/1024 (0%)]\tLoss: 0.043461\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 29 [0/1024 (0%)]\tLoss: 0.041096\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 30 [0/1024 (0%)]\tLoss: 0.038491\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 31 [0/1024 (0%)]\tLoss: 0.036244\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 32 [0/1024 (0%)]\tLoss: 0.034853\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 33 [0/1024 (0%)]\tLoss: 0.034435\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 34 [0/1024 (0%)]\tLoss: 0.034669\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 35 [0/1024 (0%)]\tLoss: 0.034630\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 36 [0/1024 (0%)]\tLoss: 0.033575\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 37 [0/1024 (0%)]\tLoss: 0.031692\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 38 [0/1024 (0%)]\tLoss: 0.029742\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 39 [0/1024 (0%)]\tLoss: 0.028269\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 40 [0/1024 (0%)]\tLoss: 0.027427\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 41 [0/1024 (0%)]\tLoss: 0.027061\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 42 [0/1024 (0%)]\tLoss: 0.026862\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 43 [0/1024 (0%)]\tLoss: 0.026555\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 44 [0/1024 (0%)]\tLoss: 0.026018\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 45 [0/1024 (0%)]\tLoss: 0.025264\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 46 [0/1024 (0%)]\tLoss: 0.024459\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 47 [0/1024 (0%)]\tLoss: 0.023877\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 48 [0/1024 (0%)]\tLoss: 0.023621\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 49 [0/1024 (0%)]\tLoss: 0.023554\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 50 [0/1024 (0%)]\tLoss: 0.023390\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 51 [0/1024 (0%)]\tLoss: 0.022979\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 52 [0/1024 (0%)]\tLoss: 0.022428\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 53 [0/1024 (0%)]\tLoss: 0.021966\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 54 [0/1024 (0%)]\tLoss: 0.021719\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 55 [0/1024 (0%)]\tLoss: 0.021616\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 56 [0/1024 (0%)]\tLoss: 0.021507\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 57 [0/1024 (0%)]\tLoss: 0.021292\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 58 [0/1024 (0%)]\tLoss: 0.020988\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 59 [0/1024 (0%)]\tLoss: 0.020694\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 60 [0/1024 (0%)]\tLoss: 0.020494\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 61 [0/1024 (0%)]\tLoss: 0.020394\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 62 [0/1024 (0%)]\tLoss: 0.020281\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 63 [0/1024 (0%)]\tLoss: 0.020073\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 64 [0/1024 (0%)]\tLoss: 0.019830\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 65 [0/1024 (0%)]\tLoss: 0.019647\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 66 [0/1024 (0%)]\tLoss: 0.019554\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 67 [0/1024 (0%)]\tLoss: 0.019479\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 68 [0/1024 (0%)]\tLoss: 0.019359\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 69 [0/1024 (0%)]\tLoss: 0.019190\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 70 [0/1024 (0%)]\tLoss: 0.019017\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 71 [0/1024 (0%)]\tLoss: 0.018869\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 72 [0/1024 (0%)]\tLoss: 0.018729\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 73 [0/1024 (0%)]\tLoss: 0.018561\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 74 [0/1024 (0%)]\tLoss: 0.018353\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 75 [0/1024 (0%)]\tLoss: 0.018139\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 76 [0/1024 (0%)]\tLoss: 0.017949\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 77 [0/1024 (0%)]\tLoss: 0.017777\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 78 [0/1024 (0%)]\tLoss: 0.017598\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 79 [0/1024 (0%)]\tLoss: 0.017404\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 80 [0/1024 (0%)]\tLoss: 0.017222\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 81 [0/1024 (0%)]\tLoss: 0.017060\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 82 [0/1024 (0%)]\tLoss: 0.016891\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 83 [0/1024 (0%)]\tLoss: 0.016706\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 84 [0/1024 (0%)]\tLoss: 0.016523\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 85 [0/1024 (0%)]\tLoss: 0.016346\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 86 [0/1024 (0%)]\tLoss: 0.016156\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 87 [0/1024 (0%)]\tLoss: 0.015951\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 88 [0/1024 (0%)]\tLoss: 0.015752\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 89 [0/1024 (0%)]\tLoss: 0.015551\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 90 [0/1024 (0%)]\tLoss: 0.015337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Training the Unet Model **************\n",
      "Train Epoch: 91 [0/1024 (0%)]\tLoss: 0.015121\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 92 [0/1024 (0%)]\tLoss: 0.014912\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 93 [0/1024 (0%)]\tLoss: 0.014696\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 94 [0/1024 (0%)]\tLoss: 0.014468\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 95 [0/1024 (0%)]\tLoss: 0.014243\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 96 [0/1024 (0%)]\tLoss: 0.014012\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 97 [0/1024 (0%)]\tLoss: 0.013766\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 98 [0/1024 (0%)]\tLoss: 0.013500\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 99 [0/1024 (0%)]\tLoss: 0.013246\n",
      "********* Training the Unet Model **************\n",
      "Train Epoch: 100 [0/1024 (0%)]\tLoss: 0.013044\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from logger import Logger\n",
    "import pdb\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "epochs = 100\n",
    "log_interval = 10\n",
    "model_folder = 'small-unet'\n",
    "\n",
    "model = UNet()\n",
    "loss_function = F.mse_loss\n",
    "optimizer = optim.Adam(model.parameters(), amsgrad=True, lr=0.001)\n",
    "logger = Logger('./logs/' + model_folder)\n",
    "\n",
    "def train_Unet(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        rgb, depth = data['image'], data['depth']\n",
    "        optimizer.zero_grad()\n",
    "        output = model(rgb)\n",
    "        target = depth[:,0,:,:].view(list(depth.shape)[0], 1, output_height, output_width)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(rgb), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        if epoch % 10 == 0:\n",
    "            F = plt.figure(1, (30, 60))\n",
    "            F.subplots_adjust(left=0.05, right=0.95)\n",
    "            plot_grid(F, rgb, target, output, batch_size)\n",
    "            plt.show()\n",
    "        if batch_idx == 0: break\n",
    "\n",
    "def validate_Unet():\n",
    "    print('validating unet')\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(val_loader):\n",
    "            rgb, depth = data['image'], data['depth']\n",
    "            output = model(rgb)\n",
    "            target = depth[:,0,:,:].view(list(depth.shape)[0], 1, output_height, output_width)\n",
    "            validation_loss += F.mse_loss(output, target)\n",
    "        validation_loss /= batch_idx\n",
    "        print('\\nValidation set: Average loss: {:.6f} {:.6f} {:.6f}\\n'.format(validation_loss))\n",
    "\n",
    "folder_name = \"models/\" + model_folder\n",
    "if not os.path.exists(folder_name): os.mkdir(folder_name)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"********* Training the Unet Model **************\")\n",
    "    train_Unet(epoch)\n",
    "    if epoch % 25== 0:\n",
    "        model_file = folder_name + \"/\" + 'model_' + str(epoch) + '.pth'\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "#    validate_Unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:\n",
      "tensor([[[[1.0020, 1.0046, 1.0299,  ..., 0.8161, 0.7773, 0.7794],\n",
      "          [0.9916, 0.9909, 0.9994,  ..., 0.7719, 0.7768, 0.7795],\n",
      "          [0.9869, 0.9793, 0.9828,  ..., 0.7678, 0.7797, 0.7832],\n",
      "          ...,\n",
      "          [0.7032, 0.7053, 0.7219,  ..., 0.6852, 0.6584, 0.6599],\n",
      "          [0.6525, 0.6451, 0.6386,  ..., 0.6874, 0.6606, 0.6618],\n",
      "          [0.6252, 0.6051, 0.5937,  ..., 0.6794, 0.6641, 0.6635]]]])\n",
      "noisy depth:\n",
      "tensor([[[[1.1020, 1.1046, 1.1299,  ..., 0.9161, 0.8773, 0.8794],\n",
      "          [1.0916, 1.0909, 1.0994,  ..., 0.8719, 0.8768, 0.8795],\n",
      "          [1.0869, 1.0793, 1.0828,  ..., 0.8678, 0.8797, 0.8832],\n",
      "          ...,\n",
      "          [0.8032, 0.8053, 0.8219,  ..., 0.7852, 0.7584, 0.7599],\n",
      "          [0.7525, 0.7451, 0.7386,  ..., 0.7874, 0.7606, 0.7618],\n",
      "          [0.7252, 0.7051, 0.6937,  ..., 0.7794, 0.7641, 0.7635]]]])\n",
      "cust loss 0.005000\n",
      "mse loss 0.010000\n",
      "l1 loss 0.099999\n",
      "smooth l1 loss 0.005000\n",
      "depth:\n",
      "tensor([[[[0.6341, 0.6316, 0.6214,  ..., 0.3574, 0.3218, 0.3185],\n",
      "          [0.6356, 0.6326, 0.6243,  ..., 0.3376, 0.3011, 0.3049],\n",
      "          [0.6390, 0.6349, 0.6288,  ..., 0.3252, 0.2979, 0.2978],\n",
      "          ...,\n",
      "          [0.5859, 0.6637, 0.7030,  ..., 0.1240, 0.1210, 0.1233],\n",
      "          [0.5509, 0.5411, 0.6908,  ..., 0.1236, 0.1277, 0.1263],\n",
      "          [0.5321, 0.5248, 0.6945,  ..., 0.1295, 0.1289, 0.1281]]]])\n",
      "noisy depth:\n",
      "tensor([[[[0.7341, 0.7316, 0.7214,  ..., 0.4574, 0.4218, 0.4185],\n",
      "          [0.7356, 0.7326, 0.7243,  ..., 0.4376, 0.4011, 0.4049],\n",
      "          [0.7390, 0.7349, 0.7288,  ..., 0.4252, 0.3979, 0.3978],\n",
      "          ...,\n",
      "          [0.6859, 0.7637, 0.8030,  ..., 0.2240, 0.2210, 0.2233],\n",
      "          [0.6509, 0.6411, 0.7908,  ..., 0.2236, 0.2277, 0.2263],\n",
      "          [0.6321, 0.6248, 0.7945,  ..., 0.2295, 0.2289, 0.2281]]]])\n",
      "cust loss 0.005000\n",
      "mse loss 0.010000\n",
      "l1 loss 0.099999\n",
      "smooth l1 loss 0.005000\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(train_loader):\n",
    "    rgb, depth = data['image'], data['depth']\n",
    "    noisy_depth = depth + 0.1\n",
    "    loss = custom_loss_function(depth, noisy_depth)\n",
    "    print(\"depth:\")\n",
    "    print(depth)\n",
    "    print(\"noisy depth:\")\n",
    "    print(noisy_depth)\n",
    "    print(\"cust loss %.6f\" % loss)\n",
    "    print(\"mse loss %.6f\" % F.mse_loss(depth, noisy_depth))\n",
    "    print(\"l1 loss %.6f\" % F.l1_loss(depth, noisy_depth))\n",
    "    print(\"smooth l1 loss %.6f\" % F.smooth_l1_loss(depth, noisy_depth))\n",
    "    if batch_idx == 1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
