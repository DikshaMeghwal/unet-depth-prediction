{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import zipfile\n",
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "rgb_data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),    # Different for Input Image & Depth Image\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629)) # Calculate this statistics for the training image.\n",
    "])\n",
    "\n",
    "depth_data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),    # Different for Input Image & Depth Image\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629)) # Calculate this statistics for the training image.\n",
    "])\n",
    "\n",
    "input_for_plot_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),    # Different for Input Image & Depth Image\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629)) # Calculate this statistics for the training image.\n",
    "])\n",
    "\n",
    "def initialize_data(folder):\n",
    "    rgb_images = folder + '/rgb'\n",
    "    if not os.path.isdir(rgb_images):\n",
    "        raise(RuntimeError(\"Could not found {}/rgb folder\".format(folder)))\n",
    "\n",
    "    depth_images = folder + '/depth'\n",
    "    if not os.path.isdir(depth_images):\n",
    "        raise(RuntimeError(\"Could not found {}/depth folder\".format(folder)))\n",
    "\n",
    "    # Total Image - 1449 (Division: Train-ing - 1024, Validation - 256, Testing - 169)\n",
    "    dataset_prepared = True\n",
    "\n",
    "    train_folder = folder + '/train_images'\n",
    "    if not os.path.isdir(train_folder):\n",
    "        dataset_prepared = False\n",
    "        os.mkdir(train_folder)\n",
    "        os.mkdir(train_folder + '/rgb')\n",
    "        os.mkdir(train_folder + '/depth')\n",
    "\n",
    "    val_folder = folder + '/val_images'\n",
    "    if not os.path.isdir(val_folder):\n",
    "        os.mkdir(val_folder)\n",
    "        os.mkdir(val_folder + '/rgb')\n",
    "        os.mkdir(val_folder + '/depth')\n",
    "\n",
    "    test_folder = folder + '/test_images'\n",
    "    if not os.path.isdir(test_folder):\n",
    "        os.mkdir(test_folder)\n",
    "        os.mkdir(test_folder + '/rgb')\n",
    "        os.mkdir(test_folder + '/depth')\n",
    "\n",
    "    if not dataset_prepared:\n",
    "        for f in os.listdir(rgb_images):\n",
    "            image_no = int(f.split(\".\")[0])\n",
    "            if image_no < 1024:\n",
    "                dest_folder = train_folder\n",
    "            elif image_no < 1280:       # 1024 + 256\n",
    "                dest_folder = val_folder\n",
    "            else:\n",
    "                dest_folder = test_folder\n",
    "            os.rename(rgb_images + '/' + f, dest_folder + '/rgb' + '/' + f)\n",
    "        for f in os.listdir(depth_images):\n",
    "            image_no = int(f.split(\".\")[0])\n",
    "            if image_no < 1024:\n",
    "                dest_folder = train_folder\n",
    "            elif image_no < 1280:       # 1024 + 256\n",
    "                dest_folder = val_folder\n",
    "            else:\n",
    "                dest_folder = test_folder\n",
    "            os.rename(depth_images + '/' + f, dest_folder + '/depth' + '/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.conv(x))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=3, activation=F.relu, space_dropout=False):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_size, out_size, 2, stride=2)\n",
    "        self.conv = nn.Conv2d(in_size, out_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_size, out_size, kernel_size, padding=1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        out = torch.cat([up, bridge], 1)\n",
    "        out = self.activation(self.conv(out))\n",
    "        out = self.activation(self.conv2(out))\n",
    "        return out\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.activation = F.tanh\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv_block1_64 = UNetConvBlock(3, 64)\n",
    "        self.conv_block64_128 = UNetConvBlock(64, 128)\n",
    "        self.conv_block128_256 = UNetConvBlock(128, 256)\n",
    "        self.conv_block256_512 = UNetConvBlock(256, 512)\n",
    "        self.conv_block512_1024 = UNetConvBlock(512, 1024)\n",
    "\n",
    "        self.up_block1024_512 = UNetUpBlock(1024, 512)\n",
    "        self.up_block512_256 = UNetUpBlock(512, 256)\n",
    "        self.up_block256_128 = UNetUpBlock(256, 128)\n",
    "        self.up_block128_64 = UNetUpBlock(128, 64)\n",
    "\n",
    "        self.last = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1_64(x)\n",
    "        pool1 = self.pool1(block1)\n",
    "\n",
    "        block2 = self.conv_block64_128(pool1)\n",
    "        pool2 = self.pool2(block2)\n",
    "\n",
    "        block3 = self.conv_block128_256(pool2)\n",
    "        pool3 = self.pool3(block3)\n",
    "\n",
    "        block4 = self.conv_block256_512(pool3)\n",
    "        pool4 = self.pool4(block4)\n",
    "\n",
    "        block5 = self.conv_block512_1024(pool4)\n",
    "\n",
    "        up1 = self.up_block1024_512(block5, block4)\n",
    "\n",
    "        up2 = self.up_block512_256(up1, block3)\n",
    "\n",
    "        up3 = self.up_block256_128(up2, block2)\n",
    "\n",
    "        up4 = self.up_block128_64(up3, block1)\n",
    "\n",
    "        return self.last(up4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images):\n",
    "    grid = utils.make_grid(images)\n",
    "    plt.imshow(grid.cpu().detach().numpy().transpose((1, 2, 0)))\n",
    "    plt.show();\n",
    "\n",
    "def format_data_for_display(tensor):\n",
    "    maxVal = tensor.max()\n",
    "    minVal = abs(tensor.min())\n",
    "    maxVal = max(maxVal,minVal)\n",
    "    output_data = tensor / maxVal\n",
    "    output_data = output_data / 2\n",
    "    output_data = output_data + 0.5\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Training the Unet Model **************\n",
      "depth:\n",
      "tensor([[[[0.5294, 0.5294, 0.5294,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          [0.5294, 0.5294, 0.5294,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          [0.5294, 0.5294, 0.5255,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          ...,\n",
      "          [0.2235, 0.2157, 0.1725,  ..., 0.1451, 0.1569, 0.1569],\n",
      "          [0.2196, 0.2196, 0.2196,  ..., 0.1569, 0.1569, 0.1569],\n",
      "          [0.2196, 0.2196, 0.2196,  ..., 0.1569, 0.1569, 0.1569]],\n",
      "\n",
      "         [[0.5294, 0.5294, 0.5294,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          [0.5294, 0.5294, 0.5294,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          [0.5294, 0.5294, 0.5255,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          ...,\n",
      "          [0.2235, 0.2157, 0.1725,  ..., 0.1451, 0.1569, 0.1569],\n",
      "          [0.2196, 0.2196, 0.2196,  ..., 0.1569, 0.1569, 0.1569],\n",
      "          [0.2196, 0.2196, 0.2196,  ..., 0.1569, 0.1569, 0.1569]],\n",
      "\n",
      "         [[0.5294, 0.5294, 0.5294,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          [0.5294, 0.5294, 0.5294,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          [0.5294, 0.5294, 0.5255,  ..., 0.2471, 0.2471, 0.2471],\n",
      "          ...,\n",
      "          [0.2235, 0.2157, 0.1725,  ..., 0.1451, 0.1569, 0.1569],\n",
      "          [0.2196, 0.2196, 0.2196,  ..., 0.1569, 0.1569, 0.1569],\n",
      "          [0.2196, 0.2196, 0.2196,  ..., 0.1569, 0.1569, 0.1569]]]])\n",
      "output:\n",
      "tensor([[[[-0.0907, -0.0907, -0.0882,  ..., -0.0879, -0.0877, -0.0876],\n",
      "          [-0.0877, -0.0908, -0.0872,  ..., -0.0865, -0.0882, -0.0881],\n",
      "          [-0.0929, -0.1002, -0.0921,  ..., -0.0913, -0.0914, -0.0862],\n",
      "          ...,\n",
      "          [-0.0906, -0.1004, -0.0931,  ..., -0.0922, -0.0898, -0.0879],\n",
      "          [-0.0866, -0.0918, -0.0906,  ..., -0.0900, -0.0911, -0.0876],\n",
      "          [-0.0858, -0.0817, -0.0836,  ..., -0.0811, -0.0819, -0.0824]]]],\n",
      "       grad_fn=<ThnnConv2DBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 0.530997\n",
      "********* Training the Unet Model **************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2d0972aa51c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"********* Training the Unet Model **************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mtrain_Unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;31m#     validate_Unet()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m#     model_file = folder_name + \"/\" + 'model_' + str(epoch) + '.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2d0972aa51c3>\u001b[0m in \u001b[0;36mtrain_Unet\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-00278b096ee9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mblock1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block1_64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-00278b096ee9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 308\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from logger import Logger\n",
    "import pdb\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Training settings\n",
    "# parser = argparse.ArgumentParser(description='PyTorch depth map prediction example')\n",
    "# parser.add_argument('model_folder', type=str, default='trial', metavar='F',\n",
    "#                     help='In which folder do you want to save the model')\n",
    "# parser.add_argument('--data', type=str, default='data', metavar='D',\n",
    "#                     help=\"folder where data is located. train_data.zip and test_data.zip need to be found in the folder\")\n",
    "# parser.add_argument('--batch-size', type = int, default = 32, metavar = 'N',\n",
    "#                     help='input batch size for training (default: 8)')\n",
    "# parser.add_argument('--epochs', type=int, default = 10, metavar='N',\n",
    "#                     help='number of epochs to train (default: 10)')\n",
    "# parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "#                     help='learning rate (default: 0.01)')\n",
    "# parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "#                     help='SGD momentum (default: 0.5)')\n",
    "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                     help='random seed (default: 1)')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='how many batches to wait before logging training status')\n",
    "# parser.add_argument('--suffix', type=str, default='', metavar='D',\n",
    "#                     help='suffix for the filename of models and output files')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "data = 'data'\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "lr = 0.0001\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "suffix = ''\n",
    "model_folder = 'unet-image-size-const'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "output_height = 256 \n",
    "output_width = 256\n",
    "\n",
    "### Data Initialization and Loading\n",
    "initialize_data(data) # extracts the zip files, makes a validation set\n",
    "\n",
    "train_rgb_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data + '/train_images/rgb/', transform = rgb_data_transforms), batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "train_depth_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data + '/train_images/depth/', transform = depth_data_transforms), batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "val_rgb_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data + '/val_images/rgb/', transform = rgb_data_transforms), batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "val_depth_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data + '/val_images/depth/', transform = depth_data_transforms), batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "model = UNet()\n",
    "loss_function = F.mse_loss\n",
    "#loss_function = F.smooth_l1_loss\n",
    "# loss_function = F.l1_loss\n",
    "optimizer = optim.Adam(model.parameters(), amsgrad=True, lr=0.0001)\n",
    "dtype=torch.cuda.FloatTensor\n",
    "logger = Logger('./logs/' + model_folder)\n",
    "\n",
    "def train_Unet(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (rgb, depth) in enumerate(zip(train_rgb_loader, train_depth_loader)):\n",
    "        rgb, depth = rgb[0], depth[0]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(rgb)\n",
    "        target = depth[:,0,:,:].view(list(depth.size())[0], 1, output_height, output_width)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(\"rgb:\")\n",
    "#         print(rgb)\n",
    "#         display_images(format_data_for_display(rgb))\n",
    "        print(\"depth:\")\n",
    "#         display_images(format_data_for_display(depth))\n",
    "        print(depth)\n",
    "        print(\"output:\")\n",
    "        print(output)\n",
    "        display_images(format_data_for_display(output))\n",
    "        if batch_idx % log_interval == 0:\n",
    "#             training_tag = \"training loss epoch:\" + str(epoch)\n",
    "#             logger.scalar_summary(training_tag, loss.item(), batch_idx)\n",
    "\n",
    "#             for tag, value in model.named_parameters():\n",
    "#                 tag = tag.replace('.', '/') + \":\" + str(epoch)\n",
    "#                 logger.histo_summary(tag, value.data.numpy(), batch_idx)\n",
    "#                 logger.histo_summary(tag + '/grad', value.grad.data.numpy(), batch_idx)\n",
    "\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(rgb), len(train_rgb_loader.dataset),\n",
    "                100. * batch_idx / len(train_rgb_loader), loss.item()))\n",
    "#         batch_idx = batch_idx + 1\n",
    "        if batch_idx == 0: break\n",
    "\n",
    "def validate_Unet():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(rgb, depth) in enumerate(zip(train_rgb_loader, train_depth_loader)):\n",
    "            rgb, depth = rgb[0], depth[0]\n",
    "            output = model(rgb)\n",
    "            target = depth[:,0,:,:].view(batch_size, 1, output_height, output_width)\n",
    "            validation_loss += loss_function(output, target)\n",
    "    #         if batch_idx == 2: break\n",
    "        validation_loss /= batch_idx\n",
    "        logger.scalar_summary(\"validation loss\", validation_loss, epoch)\n",
    "        print('\\nValidation set: Average loss: {:.4f} \\n'.format(validation_loss))\n",
    "\n",
    "folder_name = \"models/\" + model_folder\n",
    "if not os.path.exists(folder_name): os.mkdir(folder_name)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"********* Training the Unet Model **************\")\n",
    "    train_Unet(epoch)\n",
    "#     validate_Unet()\n",
    "#     model_file = folder_name + \"/\" + 'model_' + str(epoch) + '.pth'\n",
    "#     torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31031745\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for parameter in model.parameters():\n",
    "    count += (parameter.numel())\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error1:9.99998956104e-05\n",
      "error2:5.00124297105e-05\n",
      "error3:0.0100050121546\n",
      "error4:0.903911232948\n",
      "error5:0.0130405314267\n",
      "error1:9.99998446787e-05\n",
      "error2:5.00124297105e-05\n",
      "error3:0.0100050121546\n",
      "error4:-0.0511762723327\n",
      "error5:0.030722219497\n",
      "error1:9.99999756459e-05\n",
      "error2:5.00124369864e-05\n",
      "error3:0.0100050121546\n",
      "error4:-0.0824354141951\n",
      "error5:0.604435503483\n",
      "error1:9.9999961094e-05\n",
      "error2:5.00124333485e-05\n",
      "error3:0.0100050121546\n",
      "error4:0.0579924061894\n",
      "error5:1.37280464172\n",
      "error1:9.99998810585e-05\n",
      "error2:5.00124297105e-05\n",
      "error3:0.0100050121546\n",
      "error4:0.120071038604\n",
      "error5:0.00920666847378\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=10)\n",
    "data = 'data'\n",
    "batch_size = 1\n",
    "output_height = 256 \n",
    "output_width = 256\n",
    "\n",
    "def mse_log_error(output, target):\n",
    "    target = target + 0.000001\n",
    "    target = torch.log10(target)\n",
    "    output = output + 0.000001\n",
    "    output = torch.log10(output)\n",
    "    return F.mse_loss(output, target)\n",
    "\n",
    "def rel_error(output, target):\n",
    "    target = target + 0.000001\n",
    "    target = torch.log10(target)\n",
    "    output = output + 0.000001\n",
    "    output = torch.log10(output)\n",
    "    diff = (output-target)/target\n",
    "    return diff.mean()\n",
    "### Data Initialization and Loading\n",
    "initialize_data(data) # extracts the zip files, makes a validation set\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "\n",
    "train_rgb_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data + '/train_images/rgb/', transform = rgb_data_transforms), batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "train_depth_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data + '/train_images/depth/', transform = depth_data_transforms), batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "output_width=256\n",
    "output_height=256\n",
    "for batch_idx,(rgb, depth) in enumerate(zip(train_rgb_loader, train_depth_loader)):\n",
    "    depth = depth[0]\n",
    "    target = depth[:,0,:,:].view(list(depth.size())[0], 1, output_height, output_width)\n",
    "    noise = torch.zeros((list(depth.size())[0], 1, output_height, output_width), dtype=torch.float32) + 0.01\n",
    "    noisy_target = torch.add(target, noise)\n",
    "#     print(\"target:\")\n",
    "#     print(target)\n",
    "#     print(\"noisy target:\")\n",
    "#     print(noisy_target)\n",
    "    error1 = F.mse_loss(target, noisy_target)\n",
    "    error2 = F.smooth_l1_loss(target, noisy_target)\n",
    "    error3 = F.l1_loss(target, noisy_target)\n",
    "    error4 = rel_error(noisy_target, target)\n",
    "    error5 = mse_log_error(noisy_target, target)\n",
    "    print(\"error1:{}\".format(error1))\n",
    "    print(\"error2:{}\".format(error2))\n",
    "    print(\"error3:{}\".format(error3))\n",
    "    print(\"error4:{}\".format(error4))\n",
    "    print(\"error5:{}\".format(error5))\n",
    "    if batch_idx == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
